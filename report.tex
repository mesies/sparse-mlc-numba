\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in,marginparwidth=2in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

% clean citations
\usepackage{cite}
\usepackage[nottoc]{tocbibind}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
%\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% text layout - change as needed
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% use adjustwidth environment to exceed text width (see examples in text)
\usepackage{changepage}

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% headrule, footrule and page numbers
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}

\usepackage{color}


\definecolor{Gray}{gray}{.25}


\usepackage{graphicx}

% use if you want to put caption to the side of the figure - see example in text
\usepackage{sidecap}

% use for have text wrap around figures
\usepackage{wrapfig}
\usepackage[pscoord]{eso-pic}
\usepackage[fulladjust]{marginnote}
\reversemarginpar

% document begins here
\begin{document}
\vspace*{0.35in}
	%TODO Change title
	% title goes here:
	\begin{flushleft}
		{\Large
			\textbf\newline{Implementation of Multi-Label Classification in Sparse Matrices}
		}
		\newline
		\\
		Napoleon Maraidonis\textsuperscript{1} under the supervision of
		Michalis Titsias\textsuperscript{1}		
						
		\bigskip
		\bf{1} Department of Informatics, Athens University of Economics and Business
		\\		
	\end{flushleft}
	
	\section*{Abstract}
	
	
	% the * after section prevents numbering
	\section*{Introduction}
	
	Introduction \cite{extreme_MLC_omar} \cite{extreme_MLC_rep} \cite{MLC_finland}
	

	\section*{Notation}

	\begin{itemize}
		\item Lowercase bold letter denotes a n * k matrix where n is rows and k is columns :  $\textbf{w} \in  {\mathbb R}^{n * k}$
		
		\item Lowercase bold letter with subscript denotes the n-nth row of the matrix : $\textbf{x}_{n} \in  {\mathbb R}^{k}$
				
		\item Lowercase letter will denote a real number unless stated otherwise	:	$x \in {\mathbb R}$
		
		\item Uppercase Italics letter denotes a set : $\textit{T} $
		
		\item The letter $\hat{y}$ will denote predicted label(s).
		
		\item The letter $X$ will denote the train data matrix which will have its first column, full of ones in addition to other data, $X \in {\mathbb R}^{N * K}$ with $N$ the number of instances and $K$ the number of features.
		
		\item The letter $Y$ will denote train data true labels.
		
		\item $sigm(x) \triangleq \frac{1}{1 + exp(-x)}$
		
		%TODO Fill gradually
		

	\end{itemize}
	\newpage
	
	\section*{Classifier Chains}
	%TODO Explain BR first?
	Classifier Chains is a transformation of the problem which takes into account label dependence. Classifier Chains works in the following way \cite{MLC_finland}:\\ %TODO beter way to phrase this?
	$ 
	\hat{y}_{1} = h_{1}(X) ,  	\ 
	X = 	
	\begin{bmatrix}
		X	| \hat{y}_{1} \\
	\end{bmatrix}
	,$\\
	
	$ \hat{y}_{2} = h_{2}(X) ,  	\ 
	X = 	
	\begin{bmatrix}
	X	| \hat{y}_{2} \\
	\end{bmatrix} 
	,$
	\\
	$  	
	...  
	$
	\\
	$\hat{y}_{n} = h_{n}(X) $
	\newline
	
	Each classifier is trained using results from previous classifiers in a chain, the order which labels are predicted is %for now
	arbitary. There are more complicated schemes that aim to optimise the order that labels are predicted such as Bayes Optimal CC \cite{MLC_finland}.
	
	\section*{Binary Logistic Regression Classifier}
	Logistic Regression Classifier is a  classification algorithm described by these expressions (implying two classes): 
	
	$ \hat{y} = \underset{y \in \{0,1\} }{argmax}\  p(y|\textbf{x})
	$
	 where $
	 p(y = 1 | \textbf{x} )= sigm(\textbf{w}^T \textbf{x} )
	 $
	
	The joint distribution of both classes is $p(Y |X,\textbf{w}) = \prod_{n=1}^{N} \ p({y}_{n} | \textbf{x}_{n}, \textbf{w} )$
	 so the logarithmic likelihood is $
	 L(\textbf{w}) = \sum_{n}^{N} \ y_n \ log(sigm(\textbf{w}^T \textbf{x}_n)) + (1-y_n)\ log(1 - sigm(\textbf{w}^T \textbf{x}_n))
	 $.
	 In order to find optimal $\textbf{w}$ its cost function $-L(\textbf{w})$ will be optimized utilising stochastic gradient descent.
	 
	 \subsection*{Optimization of the cost function}
	 
	 Gradient descend is an iterative optimization algorithm which in each iteration the weights are updated according to the formula:
	 $\textbf{w}^{(k+1)} \leftarrow \textbf{w}^{k} + l\ \nabla L(\textbf{w}^{(k)}) \ $ where $l$ is the learning rate. 
	 
	 The computational complexity of calculating $\nabla L$ is increasing according to the number of training instances. In order to generalize well in any machine learning algorithm, a great number of training examples is needed and gradient descend becomes computationally prohibitive.
	 
	 Stochastic gradient descend is a variation of gradient descend as its name suggests, uses sampling in order to circumvent the computational cost.
	 
	 
	 $\nabla_{w} L(\textbf{w}) = - \textbf{x}^{T}\textbf{y}+\textbf{x}^{T}\textbf{x}\textbf{w} $
	 
	 Supposing a batch $B$ choosen randomly from the training data with a size of $B$
	 
	 The update procedure becomes like this : 
	 $\textbf{x}(b) \leftarrow randomly\ choose\  b \ training \ instances\ $(b is a list of B random indexes)
	 $\textbf{w}^{(k+1)} \leftarrow \textbf{w}^{k} + l\ \nabla_{w} L(\textbf{w}^{(k)}, \textbf{x}(b),\textbf{y}(b))$
	 
	\section*{Prototype Structure and Documentation}
	\subsection*{General Structure}
	The prototype consists of three main parts : (a) Binary Classification Algorithm, (b) Multi-Label Classification Interface (c) Score function.  More specifically :

	\subparagraph{a}Firstly, the implementation of any Binary Classification Algorithm will be required: binary logistic regression was chosen for simplicity. 
	The sparsity of the training data will be put into account while implementing the algorithm and stochastic gradient descent will be used for the optimization of the loss function.
	%TODO Another section for more specifics?
	
	\subparagraph{b}Classifier chains as explained above will be used.
	
	\subparagraph{c}Scoring will be made according to the formula:
	\begin{center}
		$accuracy \triangleq \frac{|T \cap P|}{|T \cup P |}$
	\end{center}
	%TODO Explain formula\ Citation Needed
	
	\subsubsection*{Notes}
	\begin{itemize}
		\item[-] The implementation of any Multiclass Classification Algorithm will follow the following contract : The methods train(Xtrain,Ytrain), and predict(Xtest) will be implemented.
		
		\item[-] The implementation of any Multy Label Classifiactation Interface will follow the following contract using the implemented Multiclass Classification Algorithm  : The methods train(Xtrain,Ytrain), and predict(Xtest) will be implemented.

	\end{itemize}
	
	
	
	
	
	\newpage
	
	\section*{Results}

	\section*{Discussion}
	
	% You can use the \nameref{label} command to cite supporting items in the text.
	
\section*{Acknowledgments}

%This is where your bibliography is generated. Make sure that your .bib file is actually called library.bib
\bibliography{report}

%This defines the bibliographies style. Search online for a list of available styles.
\bibliographystyle{abbrv}
	
\end{document}